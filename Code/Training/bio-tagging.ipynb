{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGCSZ8r5nr1x"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjGubTTXobFF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpAhKEWlomX9"
      },
      "outputs": [],
      "source": [
        "train_bengali = pd.read_json(\"<path-to-training-data-bengali>\")\n",
        "train_english = pd.read_json(\"<path-to-training-data-english>\")\n",
        "train_hindi = pd.read_json(\"<path-to-training-data-hindi>\")\n",
        "train_codemix = pd.read_json(\"<path-to-training-data-codemix>\")\n",
        "\n",
        "val_english = pd.read_json(\"<path-to-validation-data-english>\")\n",
        "val_hindi = pd.read_json(\"<path-to-validation-data-codemix>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rB_eebpZpVSg"
      },
      "outputs": [],
      "source": [
        "train_df = pd.concat([train_bengali,train_english,train_hindi,train_codemix], axis = 0).reset_index(drop = True)\n",
        "train_df = train_df.sample(frac = 1, random_state = 123).reset_index(drop = True)\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFpJ7LJQpdO_"
      },
      "outputs": [],
      "source": [
        "bengali_chars = r\"\\u0980-\\u09FF\"\n",
        "hindi_chars = r\"\\u0900-\\u097F\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZRctpJnpjHw"
      },
      "outputs": [],
      "source": [
        "def shorten_link(text_tokens):\n",
        "    p_tokens = []\n",
        "    for i in range(len(text_tokens)):\n",
        "        if text_tokens[i].startswith(\"http\"):\n",
        "            p_tokens.append(\"http\")\n",
        "        else:\n",
        "            p_tokens.append(text_tokens[i])\n",
        "\n",
        "    return p_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ehiQDQ_pmVK"
      },
      "outputs": [],
      "source": [
        "def link_preprocess(i,text):\n",
        "    text = re.sub(\"\\n\",\" \",text)\n",
        "    text_tokens = text.split()\n",
        "    pre_tokens = shorten_link(text_tokens)\n",
        "    if len(text_tokens) != len(pre_tokens):\n",
        "        print(f\"Error in {i}\")\n",
        "    p_text = \" \".join(pre_tokens)\n",
        "    return p_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwIpFaXepp6v"
      },
      "outputs": [],
      "source": [
        "def text_preprocess(df):\n",
        "    preprocessed_text = []\n",
        "    for i in range(len(df)):\n",
        "        text = df[\"text\"].iloc[i]\n",
        "        text = str(text) if pd.notna(text) else \"\"\n",
        "        p_t = link_preprocess(i,text)\n",
        "        preprocessed_text.append(p_t)\n",
        "\n",
        "    return preprocessed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV6Ep0IEptMA"
      },
      "outputs": [],
      "source": [
        "train_df[\"preprocessed_text\"] = text_preprocess(train_df)\n",
        "val_hindi[\"preprocessed_text\"] = text_preprocess(val_hindi)\n",
        "val_english[\"preprocessed_text\"] = text_preprocess(val_english)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOkgg692p3b_"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsfRlPYkp7Kf"
      },
      "outputs": [],
      "source": [
        "val_hindi.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1rAdDKTqG9x"
      },
      "outputs": [],
      "source": [
        "val_english.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPFkXyuOqMAi"
      },
      "outputs": [],
      "source": [
        "def text_cleaning(text):\n",
        "#     text = re.sub(r\"[^\\w\\s.]\", \"\", text)\n",
        "    text = re.sub(rf\"[^{bengali_chars}{hindi_chars}\\w\\s#@']\", \" \", text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB997dKLqT-v"
      },
      "outputs": [],
      "source": [
        "clean_text = []\n",
        "for i in range(len(train_df)):\n",
        "    c_text = text_cleaning(train_df[\"preprocessed_text\"].iloc[i])\n",
        "    clean_text.append(c_text)\n",
        "\n",
        "train_df[\"clean_text\"] = clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPxgP-2ls7GO"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kznRAjtgqa4D"
      },
      "outputs": [],
      "source": [
        "clean_text_val_hi = []\n",
        "for i in range(len(val_hindi)):\n",
        "    c_text = text_cleaning(val_hindi[\"preprocessed_text\"].iloc[i])\n",
        "    clean_text_val_hi.append(c_text)\n",
        "\n",
        "val_hindi[\"clean_text\"] = clean_text_val_hi\n",
        "val_hindi.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVY6b40UytSx"
      },
      "outputs": [],
      "source": [
        "clean_text_val_en = []\n",
        "for i in range(len(val_english)):\n",
        "    c_text = text_cleaning(val_english[\"preprocessed_text\"].iloc[i])\n",
        "    clean_text_val_en.append(c_text)\n",
        "\n",
        "val_english[\"clean_text\"] = clean_text_val_en\n",
        "val_english.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xd6QkRjqcVD"
      },
      "outputs": [],
      "source": [
        "train_df[\"claims\"] = train_df[\"claims\"].fillna(\"[]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRlvLTJnqfPT"
      },
      "outputs": [],
      "source": [
        "train_df[train_df[\"claims\"].isna() == True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS1Sk2thqhkA"
      },
      "outputs": [],
      "source": [
        "def claim_preprocessing(claims):\n",
        "    p_claims =  []\n",
        "    if(claims == []):\n",
        "        return p_claims\n",
        "    for text in claims:\n",
        "        text = re.sub(\"\\n\",\" \", text)\n",
        "        text = re.sub(rf\"[^{bengali_chars}{hindi_chars}\\w\\s#@']\", \" \", text)\n",
        "        p_claims.append(text)\n",
        "\n",
        "    return p_claims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JybfZ-_9qnzR"
      },
      "outputs": [],
      "source": [
        "processed_claim = []\n",
        "for i in range(len(train_df)):\n",
        "    claim = train_df[\"claims\"].iloc[i]\n",
        "    p_claim = claim_preprocessing(claim)\n",
        "    processed_claim.append(p_claim)\n",
        "\n",
        "train_df[\"preprocessed_claims\"] = processed_claim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z_MKvKfVXeh"
      },
      "outputs": [],
      "source": [
        "i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duJyC2amtdlo"
      },
      "outputs": [],
      "source": [
        "def claim_preprocessing_val(df):\n",
        "  processed_claim_val = []\n",
        "  for i in range(len(df)):\n",
        "      claim = df[\"claims\"].iloc[i]\n",
        "      p_claim = claim_preprocessing(claim)\n",
        "      processed_claim_val.append(p_claim)\n",
        "  return processed_claim_val\n",
        "\n",
        "val_english[\"preprocessed_claims\"] = claim_preprocessing_val(val_english)\n",
        "val_hindi[\"preprocessed_claims\"] = claim_preprocessing_val(val_hindi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBGNSvhJuYLy"
      },
      "outputs": [],
      "source": [
        "text_tokens = []\n",
        "for i in range(len(train_df)):\n",
        "    text = train_df[\"clean_text\"].iloc[i]\n",
        "    text = text.strip()\n",
        "    tokens = text.split()\n",
        "    text_tokens.append(tokens)\n",
        "\n",
        "train_df[\"text_tokens\"] = text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6vnJz3XuqkT"
      },
      "outputs": [],
      "source": [
        "def text_to_tokens(df):\n",
        "  text_tokens_val = []\n",
        "  for i in range(len(df)):\n",
        "    text = df[\"clean_text\"].iloc[i]\n",
        "    text = text.strip()\n",
        "    tokens = text.split()\n",
        "    text_tokens_val.append(tokens)\n",
        "  return text_tokens_val\n",
        "\n",
        "text_tokens_val_hi = text_to_tokens(val_hindi)\n",
        "text_tokens_val_en = text_to_tokens(val_english)\n",
        "\n",
        "val_hindi[\"text_tokens\"] = text_tokens_val_hi\n",
        "val_english[\"text_tokens\"] = text_tokens_val_en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZD_XsSczalq"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo4bgZ0Zzswk"
      },
      "outputs": [],
      "source": [
        "val_hindi.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzHGXQ4mZuWB"
      },
      "outputs": [],
      "source": [
        "def BIO_tagging(text_tokens,spans):\n",
        "    label = [0]*len(text_tokens)\n",
        "    if(spans != []):\n",
        "        for claim in spans:\n",
        "            claim_tokens = claim.split()\n",
        "            if(len(claim_tokens) > 0):\n",
        "                for i, word in enumerate(text_tokens):\n",
        "                    if word == claim_tokens[0]:\n",
        "                        if text_tokens[i : i + len(claim_tokens)] == claim_tokens:\n",
        "                            label[i] = 1\n",
        "                            label[i+1 : i + len(claim_tokens)] = [2]*(len(claim_tokens) - 1)\n",
        "                            break\n",
        "\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq3HRA3Kz2V7"
      },
      "outputs": [],
      "source": [
        "claim_label= []\n",
        "for i in range(len(train_df)):\n",
        "    text_tokens = train_df[\"text_tokens\"].iloc[i]\n",
        "    spans = train_df[\"preprocessed_claims\"].iloc[i]\n",
        "    label = BIO_tagging(text_tokens,spans)\n",
        "    claim_label.append(label)\n",
        "\n",
        "train_df[\"claim_label\"] = claim_label\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG-j2kS28y_m"
      },
      "outputs": [],
      "source": [
        "def calim_label_tagging(df):\n",
        "  claim_label_val = []\n",
        "  for i in range(len(df)):\n",
        "      text_tokens = df[\"text_tokens\"].iloc[i]\n",
        "      spans = df[\"preprocessed_claims\"].iloc[i]\n",
        "      label = BIO_tagging(text_tokens,spans)\n",
        "      claim_label_val.append(label)\n",
        "  return claim_label_val\n",
        "\n",
        "claim_label_val_hi = calim_label_tagging(val_hindi)\n",
        "claim_label_val_en = calim_label_tagging(val_english)\n",
        "\n",
        "val_hindi[\"claim_label\"] = claim_label_val_hi\n",
        "val_english[\"claim_label\"] = claim_label_val_en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74CMC1fd9Zrs"
      },
      "outputs": [],
      "source": [
        "val_hindi.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t1uaGeD9hWy"
      },
      "outputs": [],
      "source": [
        "val_english.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXK2rRq89pdO"
      },
      "outputs": [],
      "source": [
        "label_list = [\"O\", \"B-Claim\", \"I-Claim\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAinGkbz9tgP"
      },
      "outputs": [],
      "source": [
        "model_name = \"microsoft/mdeberta-v3-base\" # you can use other models also ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbOXY7RR9-nS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYMoVYgY-FVn"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(text_tokens, claim_label):\n",
        "    tokenized_inputs = tokenizer(text_tokens, truncation=True, is_split_into_words=True, max_length=512)\n",
        "\n",
        "    label = []\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            label.append(-100)\n",
        "        else:\n",
        "            label.append(claim_label[word_idx])\n",
        "\n",
        "    tokenized_inputs[\"label\"] = label\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zioKBNY1-JIx"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "train_input_ids = []\n",
        "train_attention_masks = []\n",
        "train_labels = []\n",
        "for index, row in tqdm(train_df.iterrows()):\n",
        "    text_tokens = row[\"text_tokens\"]\n",
        "    claim_label = row[\"claim_label\"]\n",
        "    tokenized_inputs = tokenize_and_align_labels(text_tokens,claim_label)\n",
        "    input = tokenized_inputs[\"input_ids\"]\n",
        "    train_input_ids.append(input)\n",
        "    train_attention_masks.append(tokenized_inputs[\"attention_mask\"])\n",
        "    label = tokenized_inputs[\"label\"]\n",
        "    train_labels.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2X03U4O-OPv"
      },
      "outputs": [],
      "source": [
        "len(train_df), len(train_input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv5sKloz-gQy"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels_val(df):\n",
        "  val_input_ids = []\n",
        "  val_attention_masks = []\n",
        "  val_labels = []\n",
        "  for index, row in tqdm(df.iterrows()):\n",
        "      text_tokens = row[\"text_tokens\"]\n",
        "      claim_label = row[\"claim_label\"]\n",
        "      tokenized_inputs = tokenize_and_align_labels(text_tokens,claim_label)\n",
        "      input = tokenized_inputs[\"input_ids\"]\n",
        "      val_input_ids.append(input)\n",
        "      val_attention_masks.append(tokenized_inputs[\"attention_mask\"])\n",
        "      label = tokenized_inputs[\"label\"]\n",
        "      val_labels.append(label)\n",
        "  return val_input_ids, val_attention_masks, val_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2afya9W-32H"
      },
      "outputs": [],
      "source": [
        "val_input_ids_en, val_attention_masks_en, val_labels_en = tokenize_and_align_labels_val(val_english)\n",
        "len(val_english), len(val_input_ids_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2hY34WjAGLf"
      },
      "outputs": [],
      "source": [
        "val_input_ids_hi, val_attention_masks_hi, val_labels_hi = tokenize_and_align_labels_val(val_hindi)\n",
        "len(val_hindi), len(val_input_ids_hi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6IdsmksAjLz"
      },
      "outputs": [],
      "source": [
        "dict_train = {'input_ids' : train_input_ids, 'attention_mask' : train_attention_masks, 'labels' : train_labels}\n",
        "dict_val_en = {'input_ids' : val_input_ids_en, 'attention_mask' : val_attention_masks_en, 'labels' : val_labels_en}\n",
        "dict_val_hi = {'input_ids' : val_input_ids_hi, 'attention_mask' : val_attention_masks_hi, 'labels' : val_labels_hi}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o_hhezFA_aR"
      },
      "outputs": [],
      "source": [
        "data_train = pd.DataFrame(dict_train)\n",
        "data_val_en = pd.DataFrame(dict_val_en)\n",
        "data_val_hi = pd.DataFrame(dict_val_hi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxmXLoiJBWPj"
      },
      "outputs": [],
      "source": [
        "data_val = pd.concat([data_val_en,data_val_hi], axis = 0).reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UxQzLzNBGs2"
      },
      "outputs": [],
      "source": [
        "data_train.to_json('data_train.json', orient = 'records')\n",
        "data_val.to_json('data_val.json', orient = 'records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eko2Q93qBRLy"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"data_train.json\", 'r') as file:\n",
        "  data_train = json.load(file)\n",
        "\n",
        "with open(\"data_val.json\", 'r') as file:\n",
        "  data_val = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlFFWd9JBgE1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "muril_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list), id2label=id2label, label2id=label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-tl7E6JBj70"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Iq7gsFkBxXo"
      },
      "outputs": [],
      "source": [
        "def flatten(list_of_lists):\n",
        "  flattened = [val for sublist in list_of_lists for val in sublist]\n",
        "  return flattened"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vpGHjFqB0Fk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, jaccard_score, precision_score, recall_score\n",
        "\n",
        "def compute_metrics(logits_and_labels):\n",
        "      logits, labels = logits_and_labels\n",
        "      preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "      # remove -100 from labels and predictions\n",
        "      ground = [[t for t in label if t != -100] for label in labels]\n",
        "\n",
        "      # do the same for predictions whenever true label is -100\n",
        "      preds_jagged = [[p for p, t in zip(ps, ts) if t != -100] for ps, ts in zip(preds, labels)]\n",
        "\n",
        "      # flatten labels and preds\n",
        "      labels_flat = flatten(ground)\n",
        "      preds_flat = flatten(preds_jagged)\n",
        "\n",
        "      acc = accuracy_score(labels_flat, preds_flat)\n",
        "      f1 = f1_score(labels_flat, preds_flat, average='macro')\n",
        "      pre = precision_score(labels_flat, preds_flat, average='macro')\n",
        "      re = recall_score(labels_flat, preds_flat, average='macro')\n",
        "      jaccard = jaccard_score(labels_flat, preds_flat, average='macro')\n",
        "      return {\n",
        "        'accuracy': acc,\n",
        "        'precision' : pre,\n",
        "        'recall' : re,\n",
        "        'f1': f1,\n",
        "        'jaccard': jaccard,\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pfgwJlvB3q4"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    \"/content/drive/MyDrive/Claim Span/Models/DeBERTa-multilingual-BIO\",\n",
        "    eval_strategy = \"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    learning_rate= 3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSZmdRWWCMEZ"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    muril_model,\n",
        "    args,\n",
        "    train_dataset=data_train,\n",
        "    eval_dataset=data_val,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbpDv_AICTLa"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WHOwFABCXUT"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate(data_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feHfecHrDwAc"
      },
      "outputs": [],
      "source": [
        "trainer.save_model('/content/drive/MyDrive/Claim Span/Models/DeBERTa-multilingual-BIO')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ8slFpdCafv"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mECz7odtCZu7"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDPY0VjgKA9F"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\"/content/drive/MyDrive/Claim Span/Models/DeBERTa-multilingual-BIO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRgrdR8YDMAr"
      },
      "outputs": [],
      "source": [
        "def prediction(text_tokens):\n",
        "    inputs = tokenizer(text_tokens, return_tensors=\"pt\", padding=True, truncation=True, is_split_into_words=True)\n",
        "\n",
        "    inputs_dict = {key: value for key, value in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs_dict)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "    predictions = predictions[0].tolist()\n",
        "\n",
        "    return predictions, inputs.word_ids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAc34llPDP4i"
      },
      "outputs": [],
      "source": [
        "# Converting subword predictions to word predictions using word ids\n",
        "\n",
        "def subword_to_word_predictions(subword_predictions, word_ids):\n",
        "    actual_pred_labels = []\n",
        "    for w_ids, prediction in zip(word_ids, subword_predictions):\n",
        "        if w_ids[-2] != None:\n",
        "            allign_pred = [0]*(w_ids[-2] + 1)\n",
        "            for w_idx, pred in zip(w_ids, prediction):\n",
        "                if w_idx == None:\n",
        "                    continue\n",
        "                elif pred == 1:\n",
        "                    allign_pred[w_idx] = 1\n",
        "                elif pred == 2:\n",
        "                    allign_pred[w_idx] = 2\n",
        "            actual_pred_labels.append(allign_pred)\n",
        "        else:\n",
        "            allign_pred = []\n",
        "            actual_pred_labels.append(allign_pred)\n",
        "\n",
        "    return actual_pred_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKPkAKP-DTxS"
      },
      "outputs": [],
      "source": [
        "def prediction_and_alignment(df):\n",
        "  predicted_labels = []\n",
        "  word_ids = []\n",
        "  for i in range(len(df)):\n",
        "    p_labels,w_ids = prediction(df[\"text_tokens\"].iloc[i])\n",
        "    if(len(p_labels) == len(w_ids)):\n",
        "      predicted_labels.append(p_labels)\n",
        "      word_ids.append(w_ids)\n",
        "    else:\n",
        "      print(i) # Error in prediction\n",
        "\n",
        "  predictions = subword_to_word_predictions(predicted_labels, word_ids)\n",
        "\n",
        "  ground = df[\"claim_label\"].tolist()\n",
        "\n",
        "  for i in range(len(ground)):\n",
        "    if(len(ground[i]) == len(predictions[i])):\n",
        "        continue\n",
        "    else:\n",
        "      remaining_length = len(ground[i]) - len(predictions[i])\n",
        "      zero_padded = [0] * remaining_length\n",
        "      predictions[i] = predictions[i] + zero_padded\n",
        "\n",
        "  df[\"predicted_labels\"] = predictions\n",
        "\n",
        "  return predictions, ground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CckBkztDd6k"
      },
      "outputs": [],
      "source": [
        "p_ben, g_ben = prediction_and_alignment(val_bengali)\n",
        "p_eng, g_eng = prediction_and_alignment(val_english)\n",
        "p_hi, g_hi = prediction_and_alignment(val_hindi)\n",
        "p_cm, g_cm = prediction_and_alignment(val_codemix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFMQogr4D-q5"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(ground, preds):\n",
        "        # flatten labels and preds\n",
        "      labels_flat = flatten(ground)\n",
        "      preds_flat = flatten(preds)\n",
        "\n",
        "      acc = accuracy_score(labels_flat, preds_flat)\n",
        "      f1 = f1_score(labels_flat, preds_flat, average='macro')\n",
        "      pre = precision_score(labels_flat, preds_flat, average='macro')\n",
        "      re = recall_score(labels_flat, preds_flat, average='macro')\n",
        "      jaccard = jaccard_score(labels_flat, preds_flat, average='macro')\n",
        "\n",
        "      return acc,pre,re,f1,jaccard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkJH5el-LtEE"
      },
      "outputs": [],
      "source": [
        "acc, pre, re, f1, jac = compute_metrics(g_eng, p_eng)\n",
        "\n",
        "print(f\"Accuracy = {acc}, precision = {pre}, recall = {re}, F1 = {f1}, Jaccard = {jac}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0D9t7CuLxkX"
      },
      "outputs": [],
      "source": [
        "acc, pre, re, f1, jac = compute_metrics(g_hi, p_hi)\n",
        "\n",
        "print(f\"Accuracy = {acc}, precision = {pre}, recall = {re}, F1 = {f1}, Jaccard = {jac}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
